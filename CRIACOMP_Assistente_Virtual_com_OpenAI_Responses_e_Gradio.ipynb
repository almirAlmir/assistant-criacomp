{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/almirAlmir/assistant-criacomp/blob/main/CRIACOMP_Assistente_Virtual_com_OpenAI_Responses_e_Gradio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "3mWmijGJQBtk"
      },
      "outputs": [],
      "source": [
        "!pip install -q openai gradio tdqm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "LZasoi75QWUl"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "from tqdm import tqdm\n",
        "import gradio as gr\n",
        "import concurrent\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('/content/input_pdfs', exist_ok=True)"
      ],
      "metadata": {
        "id": "aaaMYFYE-5nP"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "5uJCdT_wQgzM",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
        "dir_pdfs = '/content/input_pdfs'\n",
        "pdf_files = [os.path.join(dir_pdfs, f) for f in os.listdir(dir_pdfs)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "MB_HUg0dQmq4"
      },
      "outputs": [],
      "source": [
        "def upload_single_pdf(file_path: str, vector_store_id: str):\n",
        "    file_name = os.path.basename(file_path)\n",
        "    try:\n",
        "        file_response = client.files.create(file=open(file_path, 'rb'), purpose=\"assistants\")\n",
        "        attach_response = client.vector_stores.files.create(\n",
        "            vector_store_id=vector_store_id,\n",
        "            file_id=file_response.id\n",
        "        )\n",
        "        return {\"file\": file_name, \"status\": \"success\"}\n",
        "    except Exception as e:\n",
        "        print(f\"Error with {file_name}: {str(e)}\")\n",
        "        return {\"file\": file_name, \"status\": \"failed\", \"error\": str(e)}\n",
        "\n",
        "def upload_pdf_files_to_vector_store(vector_store_id: str, pdf_files: list):\n",
        "    stats = {\"total_files\": len(pdf_files), \"successful_uploads\": 0, \"failed_uploads\": 0, \"errors\": []}\n",
        "\n",
        "    print(f\"{len(pdf_files)} PDF files to process. Uploading in parallel...\")\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        futures = {executor.submit(upload_single_pdf, file_path, vector_store_id): file_path for file_path in pdf_files}\n",
        "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(pdf_files)):\n",
        "            result = future.result()\n",
        "            if result[\"status\"] == \"success\":\n",
        "                stats[\"successful_uploads\"] += 1\n",
        "            else:\n",
        "                stats[\"failed_uploads\"] += 1\n",
        "                stats[\"errors\"].append(result)\n",
        "\n",
        "    return stats\n",
        "\n",
        "def create_vector_store(store_name: str) -> dict:\n",
        "    try:\n",
        "        vector_store = client.vector_stores.create(name=store_name)\n",
        "        details = {\n",
        "            \"id\": vector_store.id,\n",
        "            \"name\": vector_store.name,\n",
        "            \"created_at\": vector_store.created_at,\n",
        "            \"file_count\": vector_store.file_counts.completed\n",
        "        }\n",
        "        print(\"Vector store created:\", details)\n",
        "        return details\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating vector store: {e}\")\n",
        "        return {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "MRxUmNrFRnla",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ace551af-5f7f-4e8c-8009-4ac59150e9c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector store created: {'id': 'vs_67feef636f088191a4acc853820865be', 'name': 'my_vector_store', 'created_at': 1744760675, 'file_count': 0}\n",
            "25 PDF files to process. Uploading in parallel...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [00:03<00:00,  6.42it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'total_files': 25,\n",
              " 'successful_uploads': 25,\n",
              " 'failed_uploads': 0,\n",
              " 'errors': []}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "store_name = \"my_vector_store\"\n",
        "vector_store_details = create_vector_store(store_name)\n",
        "upload_pdf_files_to_vector_store(vector_store_details[\"id\"], pdf_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "LJpZuiGRQuWn"
      },
      "outputs": [],
      "source": [
        "def response_output(query, history):\n",
        "  response = client.responses.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    input=[\n",
        "        {\"role\": \"system\", \"content\": f'Você é um assistente virtual especializado em responder perguntas sobre a coordenação de Extensão do Centro de Informática (CIn/UFPE). Baseie suas respostas de forma única e exclusiva nas informações presentes nos 25 documentos pdf fornecidos. Não utilize conhecimento externo da sua base de dados ou informações que não estejam inseridas no conteúdo desses documentos. Se uma pergunta não tiver sua resposta clara no conteúdo dos documentos fornecidos, responda: (Sou um assistente da coordenação de extensão do CIn e somente posso responder perguntas relacionadas a esta coordenação ou projetos ligados a ela) Evite respostas vagas ou tentativas de adivinhar, responda explicitamente o que lhe ordenei. Em caso de incompletude ou ambiguidade na pergunta, peça para que o usuário reformule a pergunta. Mantenha um tom profissional, informativo e objetivo. Responda de forma clara e concisa, diretamente ao ponto da pergunta. Evite opiniões pessoais, linguagem coloquial excessiva ou informações irrelevantes. Com base no histórico de mensagens trocadas: {history}, juntamente as informações contidas em todos os documentos PDF fornecidos previamente. Formule sua próxima resposta.'},\n",
        "        {\"role\": \"user\", \"content\": query}\n",
        "    ],\n",
        "    tools=[\n",
        "        {\n",
        "            \"type\": \"file_search\",\n",
        "            \"vector_store_ids\": [vector_store_details['id']],\n",
        "        }\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  if isinstance(response.output[0], dict):\n",
        "\n",
        "    return response.output[0].content[0].text #Se o conteudo da resposta nao estiver na ferramenta\n",
        "  else:\n",
        "    return response.output[-1].content[0].text #Se nao for um dicionario ele provavelmente tem saida na ferramenta gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "sdLnSKI1QzFw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "0ea79427-6dc5-46de-d36e-a9e94bb297f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://11ade8a6862db6c20d.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://11ade8a6862db6c20d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://11ade8a6862db6c20d.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "demo = gr.ChatInterface(\n",
        "    response_output,\n",
        "    type=\"messages\"\n",
        ")\n",
        "##print(response_output) print pra observar os formatos da resposta\n",
        "demo.launch(share=True, debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}